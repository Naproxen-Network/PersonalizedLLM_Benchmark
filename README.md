# PersonaSteer Benchmark

ğŸ¯ **Personalized LLM Alignment Evaluation Platform**

Based on **ALOE Dataset** (COLING 2025), evaluation prompts referenced from **RLPA**.

## âœ¨ Features

- ğŸ“¤ Upload `.jsonl` dialogue logs
- ğŸ”¬ LLM-as-a-Judge: 5-dimension scoring + binary alignment
- ğŸ“Š Visualization: AL(k) curves, radar charts, comparison tables
- ğŸŒ Multi-language: Chinese, English, Korean

## ğŸš€ Quick Start

```bash
pip install -r requirements.txt
python app.py
# Visit http://localhost:5000
```

---

## ğŸ“‹ JSONL Format

### Required Fields

| Field | Type | Description |
|-------|------|-------------|
| `session_id` | string | Unique session ID |
| `user_profile` | string | User background (facts) |
| `user_personality` | string | User personality traits |
| `rounds` | array | Dialogue rounds |

### Round Structure

```json
{
  "round": 1,
  "user_message": "Hey!",
  "responses": {
    "Base": "Hello!",
    "Ours": "Hey there!"
  }
}
```

### Example

```jsonl
{"session_id":"user_001","user_profile":"22-year-old student in NYC, likes hiking","user_personality":"curious, introverted, witty","rounds":[{"round":1,"user_message":"Hey!","responses":{"Base":"Hello!","Ours":"Hey! What's up?"}}]}
```

### Validation

```
âœ… .jsonl extension
âœ… Valid JSON per line
âœ… Required fields exist
âœ… rounds non-empty
âœ… Consistent method names across rounds
```

---

## ğŸ“ Project Structure

```
WEB_BENCHMARK/
â”œâ”€â”€ app.py              # Flask app
â”œâ”€â”€ evaluator.py        # LLM-as-a-Judge
â”œâ”€â”€ translations.py     # i18n
â”œâ”€â”€ sample_data.jsonl   # Example data
â”œâ”€â”€ templates/          # HTML
â”œâ”€â”€ static/             # CSS, JS
â”œâ”€â”€ uploads/            # Uploaded files
â””â”€â”€ results/            # Evaluation results
```

## ğŸ“Š Evaluation Metrics

### Primary Metric: AL(k)
**Alignment Level at k-turn** â€” Score at round k (0-100).

### Aggregated Metrics

| Metric | Formula | Description |
|--------|---------|-------------|
| **AVG** | $\frac{1}{K} \sum_{k=1}^{K} AL(k)$ | Average alignment score |
| **Slope (b)** | $\arg\min_{b,a} \sum(b \cdot k + a - AL(k))^2$ | Improvement trend |
| **Intercept (a)** | Linear regression intercept | Initial alignment level |
| **RÂ²** | Coefficient of determination | Stability of improvement |
| **N-AL(k)** | $\frac{AL(k) - \min AL}{\max AL - \min AL}$ | Normalized AL |
| **Binary Rate** | $\frac{\sum(binary=1)}{N} \times 100\%$ | "Want to continue" rate |

### Scoring Dimensions (5 Ã— 20 = 100)

| Dimension | Key Points |
|-----------|------------|
| Style | Match user personality (extrovertâ†’lively, introvertâ†’calm) |
| Content | Relate to user's interests, profession, background |
| Naturalness | Conversational, concise, human-like |
| Personalization | Capture implicit needs, naturally integrated |
| Conversation | Drive meaningful dialogue, avoid repetition |

---

## ğŸ“ LLM-as-a-Judge Evaluation

### Mode 1: Fine-grained Scoring (0-100)

5 dimensions Ã— 20 points each. Output format:
```
Reasoning: [reason]
Style: [score]/20
Content: [score]/20  
Naturalness: [score]/20
Personalization: [score]/20
Conversation: [score]/20
Total: \boxed{[total]}
```

### Mode 2: Binary Judgment (0/1) â­ Primary

Simulates user perspective: "Do you want to continue chatting?"

**Strict criteria** â€” If ANY criterion fails, score = 0:
1. Naturalness
2. Relevance to interests  
3. Logical consistency
4. Excitement factor
5. Information value

Output: `\boxed{1}` (continue) or `\boxed{0}` (stop)

### Design References

| Source | Contribution |
|--------|--------------|
| **ALOE** | Data format, multi-turn AL(k) |
| **RLPA** | User-perspective evaluation, strict criteria |

## ğŸ“ Data Generation

```python
import json

sessions = [{
    "session_id": "user_001_session_001",
    "user_profile": "22-year-old student...",
    "user_personality": "curious, introverted...",
    "rounds": [
        {"round": 1, "user_message": "Hey!", 
         "responses": {"Base": "Hi!", "Ours": "Hey there!"}}
    ]
}]

with open('data.jsonl', 'w') as f:
    for s in sessions:
        f.write(json.dumps(s, ensure_ascii=False) + '\n')
```

## ğŸ”’ API Configuration

Edit `evaluator.py`:
```python
API_URL = "https://api.aigc369.com/v1/chat/completions"
```

## ğŸ“„ License

MIT License

---

Made with â¤ï¸ by PersonaSteer Team
